=== HEADER FILES ===

./include/command_processor.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */

#pragma once
#include <string>
#include "memory.h"
#include "rag.h"
#include "prompt_factory.h"
#include "llm_interface.h"
#include "webscraperTools.h"

class CommandProcessor {
public:
    CommandProcessor(Memory& mem, RAGPipeline& rag, LLMInterface& llm);

    // Starts a REPL loop
    void runLoop();

    // Handle a single line (used by runLoop, but also handy for tests)
    void handleCommand(const std::string& input);

    // NEW: Send query through Memory + RAG + LLM
    std::string processQuery(const std::string& input);

private:
    // Add to header
private:
    static constexpr size_t DEFAULT_MAX_QUERY_LENGTH = 10000;
    static constexpr int DEFAULT_RAG_TOP_K = 5;
    static constexpr int DEFAULT_SCRAPE_RESULTS = 3;
    
    size_t maxQueryLength = DEFAULT_MAX_QUERY_LENGTH;
    bool initialized = false;

    Memory& memory;
    RAGPipeline& rag;
    LLMInterface& llm;
    PromptFactory promptFactory;   
    WebScraperTools scraper;

    void ensureInitialized();
    std::pair<std::string, std::string> parseCommand(const std::string& input);
    void showHelp();
    void clearMemory();

    using CommandHandler = std::function<void(const std::string&)>;
    std::unordered_map<std::string, CommandHandler> commandHandlers;
    void initializeCommands();

    void handleScrape(const std::string& args);
    void handleRag(const std::string& args);
    void handleBackend(const std::string& args);

    // helpers
    static std::string trim(const std::string& s);
    static std::string lstripSlash(const std::string& s);
    static std::string toLower(std::string s);
    static bool startsWith(const std::string& s, const std::string& prefix);
};


```

./include/embedding_engine.h:
```cpp
#pragma once
#include <string>
#include <vector>
#include <unordered_map>

class EmbeddingEngine {
public:
    enum class Method {
        Simple,
        TfIdf,
        WordHash,
        External
    };

    EmbeddingEngine(Method method = Method::TfIdf);

    void setMethod(Method method);

    // Create embedding vector for text
    std::vector<float> embed(const std::string& text);

    // Save/load engine state (method + TF-IDF vocab/stats)
    bool saveState(const std::string& filepath) const;
    bool loadState(const std::string& filepath);

private:
    Method method;
    static constexpr size_t VOCAB_SIZE = 10000;
    std::vector<std::string> documents; // tracks all indexed texts
    // TF-IDF state
    std::unordered_map<std::string, float> globalTermFreq;
    std::unordered_map<std::string, size_t> documentFreq;

    // Embedding implementations
    std::vector<float> embedSimple(const std::string& text);
    std::vector<float> embedTfIdf(const std::string& text);
    std::vector<float> embedWordHash(const std::string& text);
    std::vector<float> embedExternal(const std::string& text);

    // Helpers
    std::vector<std::string> tokenize(const std::string& text) const;
    size_t hashToIndex(const std::string& term) const;
    float calculateIdf(const std::string& term) const;
    void updateVocabulary(const std::string& text);
    std::vector<float> normalizeVector(std::vector<float> vec) const;
};


```

./include/env_loader.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */


#pragma once
#include <string>

namespace EnvLoader {
    bool loadEnvFile(const std::string& filename = ".env");
}


```

./include/file_handler.h:
```cpp
#pragma once
#include <string>

class FileHandler {
public:
    FileHandler() = default;

    // Returns full path to agent_workspace with optional filename
    std::string getAgentWorkspacePath(const std::string& filename = "") const;

    // Convenience for memory file
    std::string getMemoryPath() const;

    // Returns full path to agent_workspace/rag with optional filename
    std::string getRagPath(const std::string& filename = "") const;

    // Returns just the rag directory path (no filename)
    std::string getRagDirectory() const;
};


```

./include/llm_interface.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */

#pragma once
#include <string>
#include <curl/curl.h>

enum class LLMBackend {
    Ollama,
    OpenAI
};

class LLMInterface {
public:
    LLMInterface(LLMBackend backend = LLMBackend::Ollama);
    ~LLMInterface();
    // Internal helpers
    std::string askOllama(const std::string& prompt);
    std::string askOpenAI(const std::string& prompt);

    std::string query(const std::string& prompt);
    
    // Allow switching dynamically
    void setBackend(LLMBackend backend);

private:
    CURL* curl = nullptr;
    struct curl_slist* headers = nullptr;
    LLMBackend backend;

    static size_t WriteCallback(void* contents, size_t size, size_t nmemb, void* userp) {
        ((std::string*)userp)->append((char*)contents, size * nmemb);
        return size * nmemb;
    }

};


```

./include/memory.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * Uses either Ollama local models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */

#pragma once
#include <json.hpp>
#include <string>
#include <vector>
#include <mutex>
#include <chrono>

using json = nlohmann::json;

class Memory {
private:
    std::string filepath;
    json data;

    mutable std::mutex mtx;     // protects data and dirty state
    mutable bool isDirty = false;
    mutable std::chrono::steady_clock::time_point lastSave;
    static constexpr auto AUTO_SAVE_INTERVAL = std::chrono::minutes(5);

    std::string getDefaultPath() const;
    void markDirty() const;
    void saveIfNeeded() const;
    void saveUnlocked() const;

public:
    // Constructor / Destructor
    explicit Memory(const std::string& path = "");
    ~Memory();

    // Persistence
    void load();            // loads from disk (overwrites memory)
    void save() const;      // flushes to disk if dirty
    void flush() const;     // unconditional save, clears dirty flag

    // Conversation
    void addMessage(const std::string& role, const std::string& content);
    void addMessages(const std::vector<std::pair<std::string, std::string>>& messages);
    std::vector<json> getConversation() const;
    void clear();

    // Summaries
    void setSummary(const std::string& summary);  // sets short_summary only
    std::string getSummary(bool useExtended = false) const;
    void updateSummary(const std::string& goal, const std::string& response);

    // Debug helpers
    std::string getFilePath() const { return filepath; }
    void printSummaries() const;
};


```

./include/prompt_factory.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */
#pragma once
#include <string>
#include "memory.h"
#include "rag.h"

class PromptFactory {
public:
    struct PromptConfig {
        size_t maxRecentMessages = 5;
        size_t maxContextLength = 4000;
        bool includeTimestamps = false;
        bool includeRoleLabels = true;
        std::string systemPrompt = "";
        std::string conversationSeparator = "\n";
    };

private:
    Memory& memory;
    RAGPipeline& rag;
    PromptConfig config;

public:
    // Overloaded constructors
    PromptFactory(Memory& mem, RAGPipeline& r);  
    PromptFactory(Memory& mem, RAGPipeline& r, const PromptConfig& cfg);

    void setConfig(const PromptConfig& cfg) { config = cfg; }
    PromptConfig getConfig() const { return config; }

    std::string buildConversationPrompt(const std::string& user_input,
                                        bool useExtendedSummary = false);

    std::string buildRagQueryPrompt(const std::string& query);

private:
    std::string truncateToLimit(const std::string& input, size_t maxLen) const;
};


```

./include/rag.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * Supports local embeddings and retrieval for C++ code.
 *
 * Licensed under the MIT License (see LICENSE in project root)
 */

#pragma once
#include "vector_store.h"
#include "embedding_engine.h"
#include "file_handler.h"
#include <unordered_map>
#include <string>
#include <vector>
#include <memory>
#include <set>
#include <shared_mutex>

// Represents a chunk of code (function, class, or global block)
struct CodeChunk {
    std::string fileName;
    std::string symbolName; 
    int startLine;
    int endLine;
    std::string code;
    std::vector<float> embedding; // reserved for later
};

// Core RAG pipeline manager
class RAGPipeline {
public:
    // constructor that owns the engine
    explicit RAGPipeline(std::unique_ptr<EmbeddingEngine> engine);
    
    // Initialize RAG pipeline and load any saved index
    void init(const std::string& indexPath = "");
    
    std::string query(const std::string& query);
    
    // Index a single source file
    void indexFile(const std::string& filePath);
    
    // Index all source files in a project directory
    void indexProject(const std::string& rootPath);
    
    // Retrieve top-k relevant code chunks
    std::vector<CodeChunk> retrieveRelevant(
        const std::string& query,
        const std::vector<int>& errorLines,
        int topK = 3);
    
    // Save/load the index
    void saveIndex() const;
    void saveIndex(const std::string& dbPath) const;
    void loadIndex();
    void loadIndex(const std::string& dbPath);
    
    // Reset the in-memory store
    void clear();

private:
    mutable std::shared_mutex chunksMutex;  // protects chunks and codeToChunkIndex
    // Constants
    static constexpr size_t MAX_FILE_SIZE = 10 * 1024 * 1024; // 10MB
    static constexpr size_t MAX_CHUNK_SIZE = 4096; // 4KB chunks
    static constexpr size_t MAX_CHUNKS = 10000;
    static constexpr size_t MAX_TOTAL_SIZE = 100 * 1024 * 1024; // 100MB
    
    inline static const std::set<std::string> SUPPORTED_EXTENSIONS = {
        ".txt", ".md", ".epub", ".pdf", ".cpp", ".h", ".hpp", ".c"
    };
    
    // Member variables
    std::string indexFilePath;
    std::vector<CodeChunk> chunks; 
    std::unique_ptr<EmbeddingEngine> engine; // owns engine
    VectorStore store;                       // non-owning use of engine
    std::unordered_map<std::string, size_t> codeToChunkIndex;
    
    // Helper functions
    void addChunkToIndex(CodeChunk&& chunk);
    std::string limitText(const std::string& text, size_t maxChars);
    void rebuildInternalStructures();
    void removeChunksFromPath(const std::string& rootPath);
    
    bool isSupportedExtension(const std::string& ext) {
        return SUPPORTED_EXTENSIONS.find(ext) != SUPPORTED_EXTENSIONS.end();
    }
    
    // Memory management
    size_t getCurrentMemoryUsage() const;
    void enforceMemoryLimits();
    
    // Chunking strategies
    std::vector<CodeChunk> createSmartChunks(const std::string& filePath, const std::string& content);
    std::vector<CodeChunk> chunkByParagraphs(const std::string& filePath, const std::string& content);
    std::vector<CodeChunk> chunkByFunctions(const std::string& filePath, const std::string& content);
    std::vector<CodeChunk> chunkBySize(const std::string& filePath, const std::string& content);
};

```

./include/similarity.h:
```cpp
// include/similarity.h
#ifndef SIMILARITY_H
#define SIMILARITY_H

#include <vector>

class ISimilarity {
public:
    virtual ~ISimilarity() = default;
    virtual float operator()(const std::vector<float>& a,
                             const std::vector<float>& b) const = 0;
};

class CosineSimilarity : public ISimilarity {
public:
    float operator()(const std::vector<float>& a,
                     const std::vector<float>& b) const override;
};

class EuclideanSimilarity : public ISimilarity {
public:
    float operator()(const std::vector<float>& a,
                     const std::vector<float>& b) const override;
};

class DotProductSimilarity : public ISimilarity {
public:
    float operator()(const std::vector<float>& a,
                     const std::vector<float>& b) const override;
};

class JaccardSimilarity : public ISimilarity {
public:
    float operator()(const std::vector<float>& a,
                     const std::vector<float>& b) const override;
};

#endif // SIMILARITY_H


```

./include/tools.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */
#pragma once
#include <string>
#include <vector>

class Tools {
public:
    virtual ~Tools() = default;

    // Example virtual interface for all tools
    virtual std::string summarizeText(const std::string& text, int numSentences = 3) = 0;

    // You can add generic methods, or keep them pure virtual for specialization
};

```

./include/vector_store.h:
```cpp
#pragma once
#include "similarity.h"
#include "embedding_engine.h"

#include <string>
#include <vector>
#include <utility>
#include <memory>

class VectorStore {
public:
    // non-owning pointer: RAGPipeline owns the engine via unique_ptr
    explicit VectorStore(EmbeddingEngine* engine)
        : embeddingEngine(engine) {}

    void addDocument(const std::string& text);
    void addDocuments(const std::vector<std::string>& texts);

    std::vector<std::vector<float>> embeddings;
    bool loadEmbeddings(const std::string& path);
    bool saveEmbeddings(const std::string& filepath) const;
    void enforceMemoryLimit(size_t maxMemoryBytes);
    size_t getMemoryUsage() const;

    void clear();

    void setSimilarity(std::unique_ptr<ISimilarity> sim);

    std::vector<std::pair<std::string, float>> retrieve(const std::string& query, int topK = 3);

private:
    static constexpr float SIMILARITY_THRESHOLD = 0.1f;

    std::vector<std::string> documents;

    EmbeddingEngine* embeddingEngine;  // non-owning raw pointer
    std::unique_ptr<ISimilarity> similarity =
        std::make_unique<CosineSimilarity>();
};


```

./include/webscraperTools.h:
```cpp
/*
 * Copyright (c) 2025 Steve Meierotto
 * 
 * basic_agent - AI Agent with Memory and RAG Capabilities
 * uses either Ollama lacal models or OpenAI API
 *
 * Licensed under the MIT License
 * See LICENSE file in the project root for full license text
 */

#pragma once
#include "tools.h"

class WebScraperTools : public Tools {
private:
    std::string apiKey;
    std::string cseId;

    // --- CURL helper ---
    std::string performCurlRequest(const std::string& url, const std::string& userAgent);

public:
    WebScraperTools();
    WebScraperTools(const std::string& apiKey, const std::string& cseId);
    std::string handleScrape(const std::string& query, int results=3, int snippets=3);

    // Implement/override
    std::string summarizeText(const std::string& text, int numSentences = 3) override;

    std::vector<std::string> webSearch(const std::string& query, int maxResults = 5);
    std::string fetchAndExtract(const std::string& url);
    std::string searchAndSummarize(const std::string& query, int numResults = 5, int numSentences = 3);
    bool writeSummary(const std::string& summary, const std::string& filename);
    // --- Starter API Methods ---
    
    // Reddit API: fetch top posts from a subreddit
    std::string fetchRedditPosts(const std::string& subreddit, int limit=5);

    // News API: fetch top articles for a query
    std::string fetchNewsArticles(const std::string& query, int limit=5);

    // YouTube API: fetch video titles and descriptions for a search query
    std::string fetchYouTubeVideos(const std::string& query, int limit=5);

    // Google Custom Search API (already partially used)
    std::string fetchGoogleCSEResults(const std::string& query, int numResults=5);
};

```
